import pandas as pd
import numpy as np
import keras
import glob
import matplotlib.pyplot as plt
import scipy
import seaborn as sns
from mlxtend.preprocessing import minmax_scaling
from sklearn.metrics import roc_curve, auc
import preprocess

from keras.utils.np_utils import to_categorical
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, GlobalAveragePooling2D, Input, BatchNormalization, Multiply, Activation
from keras.optimizers import RMSprop, SGD
from keras.regularizers import l2
from keras.preprocessing.image import ImageDataGenerator
from keras.utils import plot_model
from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from keras.callbacks import CSVLogger
from keras import backend as K

import os

BASE_PATH = '../data/food-101/'
# Test: BASE_PATH + /train/food_category 
# Test: BASE_PATH + /test/food_category

# find file paths
food = ['apple_pie','baby_back_ribs', 'baklava'] 
f_apple = glob.glob(BASE_PATH+'/train/'+food[0]+'/*')
f_baby = glob.glob(BASE_PATH+'/train/'+food[1]+'/*')
f_baklava = glob.glob(BASE_PATH+'/train/'+food[2]+'/*')
f_apple_test = glob.glob(BASE_PATH+'/test/'+food[0]+'/*')
f_baby_test = glob.glob(BASE_PATH+'/test/'+food[1]+'/*')
f_baklava_test = glob.glob(BASE_PATH+'/test/'+food[2]+'/*')
# total 1000 files for each category
print('Number of images per class:\n\t\ttrain\ttest \nApple_pie:\t{}\t{}\nBaby_pork_ribs:\t{}\t{}\nBaklava:\t{}\t{}'
      .format(len(f_apple),len(f_apple_test),len(f_baby),len(f_baby_test),len(f_baklava),len(f_baklava_test)))
      
# preview some images of each class
n = 7
fig, axes = plt.subplots(3,n,figsize=(20,10))

for i in range(n):
    axes[0, i].imshow(plt.imread(f_apple[i]))
    axes[0, i].set_title('apple pie')
    axes[1, i].imshow(plt.imread(f_baby[i]))
    axes[1, i].set_title('baby pork rib')
    axes[2, i].imshow(plt.imread(f_baklava[i]))
    axes[2, i].set_title('baklava')
 
 for i in range(len(f_apple)):
    h1,w1,c1 = plt.imread(f_apple[i]).shape
    h2,w2,c2 = plt.imread(f_baby[i]).shape
    h3,w3,c3 = plt.imread(f_baklava[i]).shape
    plt.scatter(h1,w1,c='r',marker='x',alpha=0.5)
    plt.scatter(h2,w2,c='c',marker='o',alpha=0.5)
    plt.scatter(h3,w3,c='b',marker='v',alpha=0.5)
plt.title('Image Size')
plt.legend(('apples','babies','baklavas'))

train_datagen = ImageDataGenerator(featurewise_center=False,
                 samplewise_center=False,
                 featurewise_std_normalization=False,
                 samplewise_std_normalization=False,
                 zca_whitening=False,
                 rotation_range=5,
                 width_shift_range=0.05,
                 height_shift_range=0.05,
                 shear_range=0.2,
                 zoom_range=0.2,
                 channel_shift_range=0.,
                 data_format='channels_first',
                 fill_mode='nearest',
                 cval=0.,
                 horizontal_flip=True,
                 vertical_flip=False,
                 rescale=1/255) #rescale to [0-1], add zoom range of 0.2x and horizontal flip
# train_generator = train_datagen.flow_from_directory(directory='../data/food-101/train', class_mode='categorical', target_size=(224,224), batch_size=64, classes=["apple_pie", "baklava", "baby_back_ribs"])
train_generator = train_datagen.flow_from_directory(
        '../data/food-101/train',
        target_size=(224,224),
        batch_size=64)  

# train_generator = train_datagen.flow_from_directory(
#         '../data/food-101/train/baby_back_ribs',
#         target_size=(224,224),
#         batch_size=64)
# train_generator = train_datagen.flow_from_directory(
#         '../data/food-101/train/baklava',
#         target_size=(224,224),
#         batch_size=64)

test_datagen = ImageDataGenerator(rescale=1/255,
                                 data_format='channels_first') # just rescale to [0-1] for testing set
# test_generator = train_datagen.flow_from_directory(directory='../data/food-101/test', class_mode='categorical', target_size=(224,224), batch_size=64, classes=["apple_pie", "baklava", "baby_back_ribs"])  
test_generator = test_datagen.flow_from_directory(
        '../data/food-101/test',
        target_size=(224,224),
        batch_size=64)
# test_generator = test_datagen.flow_from_directory(
#         '../data/food-101/test/baby_back_ribs',
#         target_size=(224,224),
#         batch_size=64)
# test_generator = test_datagen.flow_from_directory(
#         '../data/food-101/test/baklava',
#         target_size=(224,224),
#         batch_size=64)

#CNN MODEL
model = Sequential()
model.add(Conv2D(filters = 32, kernel_size = (5,5), data_format='channels_first', strides = 2, padding = 'Same', activation ='relu', input_shape = (3,224,224), kernel_initializer='he_normal'))
model.add(Conv2D(filters = 32, kernel_size = (5,5), data_format='channels_first', strides = 2, padding = 'Same', activation ='relu',kernel_initializer='he_normal'))
model.add(MaxPool2D(pool_size=(2,2),data_format='channels_first',))
model.add(Dropout(0.2))
# model.summary()
model.add(Conv2D(filters = 64, kernel_size = (3,3), data_format='channels_first' ,padding = 'Same', activation ='relu', input_shape = ((32, 28, 0)) , kernel_initializer='he_normal'))
model.add(Conv2D(filters = 64, kernel_size = (3,3),data_format='channels_first', padding = 'Same', activation ='relu',kernel_initializer='he_normal'))
model.add(MaxPool2D(pool_size=(2,2), data_format='channels_first',))
model.add(Dropout(0.2))
model.add(Conv2D(filters = 128, kernel_size = (2,2),data_format='channels_first',padding = 'Same', activation ='relu',kernel_initializer='he_normal'))
model.add(Conv2D(filters = 128, kernel_size = (2,2),data_format='channels_first',padding = 'Same', activation ='relu',kernel_initializer='he_normal'))
model.add(MaxPool2D(pool_size=(2,2), data_format='channels_first',))
model.add(Dropout(0.2))
model.add(Conv2D(filters = 256, kernel_size = (2,2),data_format='channels_first',padding = 'Same', activation ='relu',kernel_initializer='he_normal'))
model.add(Conv2D(filters = 256, kernel_size = (2,2),data_format='channels_first',padding = 'Same', activation ='relu',kernel_initializer='he_normal'))
model.add(GlobalAveragePooling2D(data_format='channels_first',))
model.add(Dense(512, activation = "relu",kernel_initializer='he_normal'))
model.add(Dropout(0.2))
model.add(Dense(101,activation = "softmax",kernel_initializer='he_normal',kernel_regularizer=l2()))

#callbacks
checkpointer = ModelCheckpoint(filepath='model.hdf5', verbose=1, save_best_only=True, save_weights_only=True)
earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=20, mode='auto')
reduceLR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, mode='auto')

optimizer = keras.optimizers.Adam()
optimizer.rescale_grad = 1/64
model.compile(optimizer=optimizer , loss = "categorical_crossentropy", metrics=["accuracy"])
model.summary()
csv_logger = CSVLogger('log.csv', append=True, separator=';')
history = model.fit_generator(train_generator,shuffle=True,
                              validation_data=test_generator, steps_per_epoch=75750/64, validation_steps=25250/64, 
                              epochs=100, callbacks=[checkpointer, reduceLR, earlystopping, csv_logger])
numpy_loss_history = numpy.array(history)
numpy.savetxt("loss_history.txt", numpy_loss_history, delimiter=",")

#Visualize the accuracy metrics
def plot_hist(history):
    f,ax = plt.subplots(2,1,figsize=(15,10))
    ax[0].plot(history.history['acc'],c='C2')
    ax[0].plot(history.history['val_acc'],c='C3')
    ax[0].set_title('Model accuracy')
    ax[0].set_ylabel('Accuracy')
    ax[0].set_xlabel('Epoch')
    ax[0].legend(['Train', 'Test'], loc='upper left')
    
    # summarize history for loss
    ax[1].plot(history.history['loss'],c='C0')
    ax[1].plot(history.history['val_loss'],c='C1')
    ax[1].set_title('Model loss')
    ax[1].set_ylabel('Loss')
    ax[1].set_xlabel('Epoch')
    ax[1].legend(['Train', 'Test'], loc='upper left')
    
plot_hist(history)

# create another generator for all test images in a single batch 
val_datagen = ImageDataGenerator(rescale=1./255)
val_generator = test_datagen.flow_from_directory(
        "../input/food102/food101/food101/test",
        target_size=(224,224),
        batch_size=750)
        
 x_test, y_test = val_generator.next()
y_pred_conf = model.predict(x_test) #return probabilities of each class
y_pred = np.argmax(y_pred_conf,axis=1)
y_label = np.argmax(y_test,axis=1)

f = open("test_accuracy.txt", "w")
f.write('Accuracy score: {:.1f}%'.format(accuracy_score(y_pred,y_label)*100))
f.close()

print('Accuracy score: {:.1f}%'.format(accuracy_score(y_pred,y_label)*100))
